{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e85263e1-6a92-455d-ab72-1342ad279991",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"ISISROOT\"]=\"/opt/conda/envs/isis/\"\n",
    "os.environ[\"ISISDATA\"]=\"/isis/data\"\n",
    "import fnmatch\n",
    "import kalasiris as isis\n",
    "from kalasiris import hi2isis, hical, histitch, spiceinit, spicefit, noproj, hijitreg, handmos, cubenorm, cam2map\n",
    "import psutil\n",
    "import re\n",
    "import shutil\n",
    "import subprocess\n",
    "from tqdm import tqdm\n",
    "JOBS = psutil.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b38cb30-f615-41a8-a76a-93ddce73df85",
   "metadata": {},
   "source": [
    "**Definition of a function to list all files with a specific keyword or extension in a defined folder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27d80d72-1d2c-4e87-8966-a24e76129065",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paths(PATH, ixt):\n",
    "    ext=f'*{ixt}*'\n",
    "    chkCase = re.compile(fnmatch.translate(ext), re.IGNORECASE)\n",
    "    files = [PATH+'/'+f for f in os.listdir(PATH) if chkCase.match(f)]\n",
    "    return(sorted(files))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8868d699-a65f-4f57-832f-4ebbb867ed8e",
   "metadata": {},
   "source": [
    "**Definition of the main function used to parallelize all the processing functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19cbc67-fd4d-423f-83b2-e3c54503d798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_1(files, JOBS, proc_fun, **kwargs):\n",
    "    from joblib import Parallel, delayed, parallel_backend\n",
    "    with parallel_backend(\"loky\", inner_max_num_threads=2):\n",
    "\n",
    "        Parallel (n_jobs=JOBS)(delayed(proc_fun)(files[i],**kwargs)\n",
    "                                for i in range(len(files)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed133147-f691-457a-805b-b14db83ce460",
   "metadata": {},
   "source": [
    "## Define user variables\n",
    "**Here we define:**\n",
    "- the source folder where the HiRISE RED CCD files are contained\n",
    "- the destination folder (if it necessary to change)\n",
    "- the maptemplated that will be used for the map projection\n",
    "- the del_temp flag used to delete intermediate files (saving a lot of space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31200b9-a0fd-4ab4-a10d-96689f826b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = '/home/jovyan/Data/ESP_068426_1985'\n",
    "destination = source\n",
    "maptemplate = '/home/jovyan/Data/PyISIS-Parallel/PyISIS-Parallel/maptemplates/CenterEquirectangularMars.map'\n",
    "ixt = 'IMG'\n",
    "oxt='tiff'\n",
    "delete = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78672ede-671e-4ddd-9fba-005737bf238f",
   "metadata": {},
   "source": [
    "# Get file list\n",
    "**Here we create a list of all CCD files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790f1353-88df-4f68-87dc-0dc0c812a5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "ccd_list = get_paths(source, ixt) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5e0244-eb22-4670-8b18-e56bc0d5d843",
   "metadata": {},
   "source": [
    "**tmp folder creation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed14535-06e3-4c2e-a5e4-64292af37aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_dir = f\"{destination}/tmp/\"\n",
    "os.makedirs(f\"{tmp_dir}\", exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0e5b16-e3f3-4f03-8781-c66124d3d7b0",
   "metadata": {},
   "source": [
    "# Parallel conversion of CCD to cub (hi2isis)\n",
    "**We define the function that convert the EDR IMG files into ISIS CUB files using ISIS *hi2isis* command**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093d906f-3520-465b-ab2c-4222f510359f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def h2i(src):\n",
    "    src_basename = os.path.basename(src).split('.'+ixt)[0]\n",
    "    dst_basename = f\"{tmp_dir}{src_basename}\"\n",
    "    L0 = dst_basename+'_lev0.cub'\n",
    "    if not os.path.isfile(L0):\n",
    "        try:\n",
    "            h2 = hi2isis(src, to=L0)            \n",
    "            return h2\n",
    "        except subprocess.CalledProcessError as err:\n",
    "            print(err.stdout)\n",
    "            print(err.stderr)\n",
    "            raise err\n",
    "            print(err)\n",
    "            return err\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa9396c-e392-4b0c-8480-997ff8ba901e",
   "metadata": {},
   "source": [
    "**Parallel execution of the h2i function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52caa008-d28d-4b39-98e9-5da6e0e332e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tqdm(total=len(ccd_list),\n",
    "             desc = 'Generating L0 cubs',\n",
    "             unit='File') as pbar:\n",
    "        filerange = len(ccd_list)\n",
    "        chunksize = round(filerange/JOBS)\n",
    "        if chunksize <1:\n",
    "            chunksize=1\n",
    "            JOBS = filerange\n",
    "        chunks = []\n",
    "        for c in chunk_creator(ccd_list, JOBS):\n",
    "            chunks.append(c)\n",
    "        for i in range(len(chunks)):\n",
    "            files = chunks[i]\n",
    "            parallel_1(files, JOBS, h2i)\n",
    "            pbar.update(len(files))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14674f8c-e112-4775-8880-c71f7f207c0b",
   "metadata": {},
   "source": [
    "## Parallel Calibration\n",
    "**We define the function that calibrate each CCD cube using ISIS *hical* command**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cbb9fe-cff1-4439-9834-fe38ecaa8a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def h2cal(src, del_tmp):\n",
    "    src_basename = os.path.basename(src).split('_lev0.cub')[0]\n",
    "    dst_basename = f\"{tmp_dir}{src_basename}\"\n",
    "    L1 = dst_basename+'_lev1.cub'       \n",
    "    if not os.path.isfile(L1):\n",
    "        try:\n",
    "            h2c = hical(src, to=L1)\n",
    "            if del_tmp:\n",
    "                os.remove(src)        \n",
    "            return h2c\n",
    "        except subprocess.CalledProcessError as err:\n",
    "            print(err.stdout)\n",
    "            print(err.stderr)\n",
    "            raise err\n",
    "            return err"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de9898c-586b-4464-af97-e4250c13db5b",
   "metadata": {},
   "source": [
    "**Parallel execution of the hical function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007817b3-8495-4f9a-82d5-49f08af9a9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "L0_list = get_paths(tmp_dir, 'lev0') \n",
    "with tqdm(total=len(L0_list),\n",
    "             desc = 'Generating L1 cubs',\n",
    "             unit='File') as pbar:\n",
    "        filerange = len(L0_list)\n",
    "        chunksize = round(filerange/JOBS)\n",
    "        if chunksize <1:\n",
    "            chunksize=1\n",
    "            JOBS = filerange\n",
    "        chunks = []\n",
    "        for c in chunk_creator(L0_list, JOBS):\n",
    "            chunks.append(c)\n",
    "        for i in range(len(chunks)):\n",
    "            files = chunks[i]\n",
    "            parallel_1(files, JOBS, h2cal, del_tmp=delete)\n",
    "            pbar.update(len(files))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2c6d49-5a66-4984-ba5f-a991f079b64c",
   "metadata": {},
   "source": [
    "## Parallel stitch\n",
    "**We define the function that stitch each CCD pair using ISIS histitch command**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1184be4-7d9e-4ef0-8ddb-b389934ad488",
   "metadata": {},
   "outputs": [],
   "source": [
    "def h2s(channel_pair, del_tmp):\n",
    "    chan_prefix = os.path.commonprefix(channel_pair)\n",
    "    chan_suffix = 'histich.cub'\n",
    "    dst = f'{chan_prefix}{chan_suffix}'\n",
    "    if not os.path.isfile(dst):\n",
    "        try:\n",
    "            h2 = histitch(balance=True, from1=channel_pair[0], from2=channel_pair[1], to=dst)\n",
    "            if del_tmp:\n",
    "                os.remove(channel_pair[0])\n",
    "                os.remove(channel_pair[1])\n",
    "            return h2\n",
    "        except subprocess.CalledProcessError as err:\n",
    "            print(err.stdout)\n",
    "            print(err.stderr)\n",
    "            raise err\n",
    "            return err"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e11f50-6660-4919-828e-fc1b28a376eb",
   "metadata": {},
   "source": [
    "**We generate a list of CCD Pairs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013ce1f0-b0e9-4019-91a8-e7829df8e2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "L1_list = get_paths(tmp_dir, 'lev1') \n",
    "# function from source hiedr2mosaic.py https://github.com/NeoGeographyToolkit/StereoPipeline/blob/master/src/asp/Tools/hiedr2mosaic.py\n",
    "import re\n",
    "prefix = os.path.commonprefix( L1_list )\n",
    "channel_files = [[None]*2 for i in range(len(L1_list)//2)]\n",
    "pattern = re.compile(r\"(\\d)_(\\d)\")\n",
    "for cub in L1_list:\n",
    "    match = re.match( pattern, cub[len(prefix):] )\n",
    "    if match:\n",
    "        ccd     = match.group(1)\n",
    "        channel = match.group(2)\n",
    "        # print ('ccd: ' + ccd + ' channel: '+ channel)\n",
    "        channel_files[int(ccd)][int(channel)] = cub\n",
    "    else:\n",
    "        raise Exception( 'Could not find a CCD and channel identifier in ' + cub )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70f5411-6f81-4f24-9b89-3bf3fbeea27a",
   "metadata": {},
   "source": [
    "**Parallel execution of the histich function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1205ae53-b884-41f0-9d47-92d3d8a80cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tqdm(total=len(channel_files),\n",
    "             desc = 'Stitching channels',\n",
    "             unit='File') as pbar:\n",
    "        filerange = len(channel_files)\n",
    "        chunksize = round(filerange/JOBS)\n",
    "        if chunksize <1:\n",
    "            chunksize=1\n",
    "            JOBS = filerange\n",
    "        chunks = []\n",
    "        for c in chunk_creator(channel_files, JOBS):\n",
    "            chunks.append(c)\n",
    "        for i in range(len(chunks)):\n",
    "            files = chunks[i]\n",
    "            parallel_1(files, JOBS, h2s, del_tmp=delete)\n",
    "            pbar.update(len(files))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cde36d6-c60e-4ad6-ab93-5cef38d24bd3",
   "metadata": {},
   "source": [
    "# Parallel Spiceinit\n",
    "**We define the function that initialize the retreive the acqusition parameters for each CCD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323a577e-5814-40ae-9a59-b58b89327679",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spice(src, web):\n",
    "    import subprocess\n",
    "    err=None\n",
    "    init = None\n",
    "    ii = 0\n",
    "    while init == None:\n",
    "        try:\n",
    "            sp = spiceinit(src, web=web)\n",
    "            init = 'Done'\n",
    "        except subprocess.CalledProcessError as err:\n",
    "            print(err.stdout)\n",
    "            print(err.stderr)\n",
    "            ii+=1\n",
    "            if ii==100:\n",
    "                break\n",
    "                raise err\n",
    "            raise err\n",
    " \n",
    "    return([sp, err])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b53a33b-30fe-49fc-b187-6279c26133b6",
   "metadata": {},
   "source": [
    "**Parallel execution of the spiceinit function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b2e00a-e915-4bae-98c1-55188bb1440b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stitched_list = get_paths(tmp_dir, 'histich') \n",
    "with tqdm(total=len(stitched_list),\n",
    "             desc = 'Spiceinit',\n",
    "             unit='File') as pbar:\n",
    "        filerange = len(stitched_list)\n",
    "        chunksize = round(filerange/JOBS)\n",
    "        if chunksize <1:\n",
    "            chunksize=1\n",
    "            JOBS = filerange\n",
    "        chunks = []\n",
    "        for c in chunk_creator(stitched_list, JOBS//2):\n",
    "            chunks.append(c)\n",
    "        for i in range(len(chunks)):\n",
    "            files = chunks[i]\n",
    "            parallel_1(files, JOBS//2, spice, web=True)\n",
    "            pbar.update(len(files))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3d81b6-0a95-4fd5-803b-f3947749b60b",
   "metadata": {},
   "source": [
    "# Parallel noproj\n",
    "**We define the function that removes the camera distortions each stitchedCCD using ISIS *noproj* command**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460aab7e-5d67-4860-b528-cd3d55d1b2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def noprj(src, mtch, del_tmp):\n",
    "    base_dir = os.path.dirname(src)\n",
    "    basename = os.path.basename(src).split('.cub')[0]\n",
    "    #tmp_dir = f\"{base_dir}/tmp_{basename}/\"\n",
    "    #os.makedirs(tmp_dir, exist_ok=True)    \n",
    "    dst_noproj = f\"{src.split('histich')[0]}noproj.cub\"\n",
    "    if not os.path.isfile(dst_noproj):\n",
    "        try:    \n",
    "            npj = noproj(src, match=mtch, source='frommatch',to=dst_noproj)\n",
    "            if del_tmp:\n",
    "                os.remove(src)\n",
    "            #noproj from=../ESP_068426_1985_RED0.histitch.cub match=../ESP_068426_1985_RED5.histitch.cub source= frommatch to=../ESP_068426_1985_RED0.noproj.cub\n",
    "            return npj\n",
    "        except subprocess.CalledProcessError as err:\n",
    "            print(err.stdout)\n",
    "            print(err.stderr)\n",
    "            raise err\n",
    "            return err"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6328465e-aeac-4523-ab94-8bfbd97e5bb6",
   "metadata": {},
   "source": [
    "**Parallel execution of the noproj function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05a9d5c-8333-4ea2-b719-fc16b17954b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_id=5\n",
    "with tqdm(total=len(stitched_list),\n",
    "             desc = 'Generating noproj',\n",
    "             unit='File') as pbar:\n",
    "        filerange = len(stitched_list)\n",
    "        chunksize = round(filerange/JOBS)\n",
    "        if chunksize <1:\n",
    "            chunksize=1\n",
    "            JOBS = filerange\n",
    "        chunks = []\n",
    "        for c in chunk_creator(stitched_list, JOBS):\n",
    "            chunks.append(c)\n",
    "        for i in range(len(chunks)):\n",
    "            files = chunks[i]\n",
    "            parallel_1(files, JOBS, noprj, mtch=stitched_list[match_id], del_tmp=delete)\n",
    "            pbar.update(len(files))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313a55c8-4942-4469-82cf-79d9c18dc953",
   "metadata": {},
   "source": [
    "# Parallel hijitreg\n",
    "**We define the function that characterize HiRISE jitter with co-registration using ISIS *hijitreg* command**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffabd0d3-f786-40b6-8962-e491d4d2bc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hiji(srcs, del_tmp):\n",
    "    src, mtch = srcs\n",
    "    basename = f\"{src.split('noproj')[0]}\"\n",
    "    flat_file = f\"{basename}flat.txt\"\n",
    "    if not os.path.isfile(flat_file):\n",
    "        try:    \n",
    "            hij = hijitreg(src, match=mtch, flat=flat_file)  \n",
    "            if del_tmp:\n",
    "                os.remove(src)\n",
    "            return hij\n",
    "        except subprocess.CalledProcessError as err:\n",
    "            print(err.stdout)\n",
    "            print(err.stderr)\n",
    "            raise err\n",
    "            return err"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a668dd91-ffcc-42f7-b54a-dfcf870980ed",
   "metadata": {},
   "source": [
    "**Parallel execution of the hijitreg function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132bad9f-1f33-45b9-89d2-cd35456fb343",
   "metadata": {},
   "outputs": [],
   "source": [
    "noproj_list = sorted(get_paths(tmp_dir, 'noproj'))\n",
    "with tqdm(total=len(noproj_list),\n",
    "             desc = 'Performing hijitreg',\n",
    "             unit='File') as pbar:\n",
    "        filerange = len(noproj_list)\n",
    "        chunksize = round(filerange/JOBS)\n",
    "        if chunksize <1:\n",
    "            chunksize=1\n",
    "            JOBS = filerange\n",
    "        chunks = []\n",
    "        for c in chunk_creator(noproj_list, JOBS):\n",
    "            chunks.append(c)\n",
    "        for i in range(len(chunks)):\n",
    "            files = chunks[i]\n",
    "            matches = [files[i+1] for i in range(len(files)-1)]\n",
    "            zipped = list(zip(files,matches))\n",
    "            parallel_1(zipped, JOBS, hiji, del_tmp=False)\n",
    "            pbar.update(len(files))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfcae10-2c54-4c7a-8b25-b30700d2761b",
   "metadata": {},
   "source": [
    "# Generating Mosaic\n",
    "**Finally, we assemble the first mosaic using the noproj cubs and associated flat_files which contains co-registration values.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b18816-6044-4483-ad3b-52aca40ab2d7",
   "metadata": {},
   "source": [
    "**Definition of a function to read flat_file (from original hieadr2mosaic.py)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892efcbf-0c4a-41dc-8654-dbe6ebb18ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function from source hiedr2mosaic.py https://github.com/NeoGeographyToolkit/StereoPipeline/blob/master/src/asp/Tools/hiedr2mosaic.py\n",
    "def read_flatfile( flat ):\n",
    "    f = open(flat,'r')\n",
    "    averages = [0.0,0.0]\n",
    "    try:\n",
    "        for line in f:\n",
    "            if line.rfind(\"Average Sample Offset:\") > 0:\n",
    "                index       = line.rfind(\"Offset:\")\n",
    "                index_e     = line.rfind(\"StdDev:\")\n",
    "                crop        = line[index+7:index_e]\n",
    "                averages[0] = float(crop)\n",
    "            elif line.rfind(\"Average Line Offset:\") > 0:\n",
    "                index       = line.rfind(\"Offset:\")\n",
    "                index_e     = line.rfind(\"StdDev:\")\n",
    "                crop        = line[index+7:index_e]\n",
    "                averages[1] = float(crop)\n",
    "    except ValueError:\n",
    "        print(\"Could not extract valid offsets from the flat file (\" +\n",
    "              flat + \"). \"\n",
    "              \"This could be because no matches were found. \"\n",
    "              \"You may need to run hijitreg manually with a \"\n",
    "              \"custom REGDEF parameter.  In order for this program \"\n",
    "              \"to complete, we are returning zeros as the offset \"\n",
    "              \"but this may result in misaligned CCDs.\")\n",
    "    return averages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d9dba7-8638-46c6-ae1f-d2a026517156",
   "metadata": {},
   "source": [
    "**Definition of a function to assemble the mosaic using noproj cubs and flat_files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685d16ee-e4e2-422c-9884-c4bb6aff8129",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mosaic(noproj_list, prefix, match_id,averages ):\n",
    "    #derived from function from source hiedr2mosaic.py https://github.com/NeoGeographyToolkit/StereoPipeline/blob/master/src/asp/Tools/hiedr2mosaic.py\n",
    "    mosaic_name = f\"{prefix}_mos_hijitreged.cub\"\n",
    "    if not os.path.isfile(mosaic_name):\n",
    "        try:        \n",
    "            shutil.copy(noproj_list[match_id], mosaic_name )\n",
    "            sample_sum = 1\n",
    "            line_sum   = 1\n",
    "            for i in range( match_id-1, -1, -1):\n",
    "\n",
    "                sample_sum  += averages[i][0]\n",
    "                line_sum    += averages[i][1]\n",
    "                handmos(noproj_list[i],\n",
    "                        mosaic=mosaic_name,\n",
    "                        outsample=str(int(round(sample_sum))),\n",
    "                        outline=str( int(round(line_sum))),\n",
    "                        priority='beneath')\n",
    "\n",
    "            sample_sum = 1\n",
    "            line_sum = 1\n",
    "            for i in range( match_id+1, len(noproj_list), 1):\n",
    "\n",
    "                sample_sum  -= averages[i-1][0]\n",
    "                line_sum    -= averages[i-1][1]\n",
    "                handmos(noproj_list[i],\n",
    "                        mosaic=mosaic_name,\n",
    "                        outsample=str(int(round(sample_sum))),\n",
    "                        outline=str( int(round(line_sum))),\n",
    "                        priority='beneath')\n",
    "            return mosaic_name\n",
    "        except subprocess.CalledProcessError as err:\n",
    "            print(err.stdout)\n",
    "            print(err.stderr)\n",
    "            raise err\n",
    "            return err\n",
    "    else:\n",
    "        print('Already Processed')\n",
    "        return mosaic_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2928327c-f503-4bbe-849a-3e1b6c8704bc",
   "metadata": {},
   "source": [
    "**Creation of flat_files list, to be used with mosaic function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035e051c-c26c-4bed-b816-3d499e001ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list = sorted(get_paths(source, 'flat'))\n",
    "averages = dict()\n",
    "for i in range(len(flat_list)):\n",
    "    averages[i] = read_flatfile(flat_list[i] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695624bf-701a-48cc-a6fd-bdd3408bb17e",
   "metadata": {},
   "source": [
    "**Generation of the first mosaic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9a6470-5e0f-4470-91eb-113e07823079",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mosaicked = mosaic( noproj_list, prefix=os.path.commonprefix(ccd_list),match_id=match_id, averages=averages )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf2fed1-dac0-4aa4-b30f-30cb9cd7b66b",
   "metadata": {},
   "source": [
    "**Mosaic Normalization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b958f07d-ac49-4705-b387-9de9e6512a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_norm = f\"{os.path.commonprefix(ccd_list)}_norm.cub\"\n",
    "if not os.path.isfile(dst_norm):\n",
    "    cubenorm(mosaicked, to=dst_norm)\n",
    "else:\n",
    "    print('Already Processed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf730de-5ad2-4d14-89fb-502eb4504856",
   "metadata": {},
   "source": [
    "# Cleanup of temporary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc87bd2-4115-4177-a7ca-af82bd01b209",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(tmp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e088ef23-0edb-4951-8ec7-ab711a6c10ca",
   "metadata": {},
   "source": [
    "# Map projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf6a08b-4877-40d2-9c81-5f0c5536ff39",
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_map = f\"{dst_norm.split('.cub')[0]}_map.cub\"\n",
    "if not os.path.isfile(dst_map):\n",
    "    try:    \n",
    "        cam2map(dst_norm, to=dst_map, map=maptemplate)\n",
    "        os.remove(dst_norm)\n",
    "    except subprocess.CalledProcessError as err:\n",
    "        print(err.stdout)\n",
    "        print(err.stderr)\n",
    "        raise err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea393e30-6cb5-4a0f-92d3-bddc8de500f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.KalaUtils import L2toStd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30aa2653-cae6-49c0-9ab3-5b32cb1a88e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_std = f\"{dst_norm.split('cub')[0]}tiff\"\n",
    "if not os.path.isfile(dst_std):\n",
    "    try:\n",
    "        L2toStd(dst_map, dst_std, byte='N')\n",
    "    except Exception as err:\n",
    "        print(err)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
